# Parameters for archive
# Path to a file containing prompt for generatition or a list of types in specific dimension (each type in a new line)
archive: 
  # path: [
  #   ./configs/prompts/categories.txt,
  #   ./configs/prompts/styles.txt
  # ]
  path: [
    ./configs/categories/categories.txt,
    ./configs/styles/styles.txt
  ]
  descriptor: ["Risk Category", "Attack Style"]
  generation: False # Automatically generate prompts based on prompt in path or not
  examples: ./configs/examples/examples.json # Path to json file containing examples for mutation (if needed, or leave blank)
  

sample_prompts: ./data/AdversarialQA.json

# LLM for generating dimensions of archives
init_llm:
  type_: openai # or vllm
  api_key: # YOUR OPEN API HERE

  model_kwargs:
    model: gpt-4o-2024-08-06
    max_model_len: 4096 

  sampling_params:
    temperature: 0.6
    top_p: 0.9
    max_tokens: 1024

# LLM that be attacked
target_llm:
  type_: vllm # or openai

  model_kwargs:
    model: Qwen/Qwen2.5-7B-Instruct
    trust_remote_code: True
    max_model_len: 4096 
    gpu_memory_utilization: 0.5

  sampling_params:
    temperature: 0.6
    top_p: 0.9
    max_tokens: 1024

# LLM that mutates a prompt via descriptors in the archive
mutator_llm: 
  type_: vllm

  model_kwargs:
    model: Qwen/Qwen2.5-3B-Instruct
    trust_remote_code: True
    gpu_memory_utilization: 0.3
    max_model_len: 2048

  sampling_params:
    temperature: 0.7
    top_p: 0.9
    max_tokens: 128

# LLM that judges the adversarial quality of a prompt (in case of using Rainbow algorithm)
judge_llm: None

# LLM as fitness function to evaluate multiple prompts based the probability of output
fitness_llm: 
  type_: vllm

  model_kwargs:
    model: meta-llama/Llama-Guard-3-1B
    trust_remote_code: True
    gpu_memory_utilization: 0.15
    max_model_len: 4096

  sampling_params:
    temperature: 0.7
    top_p: 0.9
    max_tokens: 16
    logprobs: 1
